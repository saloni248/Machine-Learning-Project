# -*- coding: utf-8 -*-
"""MLM_Project_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uy8-WzVPqO7_d2bzQtknvnF_vEwHgI4R

# Project Information:

 Title: **Build Unsupervised Learning Models using Python**


Name: **Priyanka Goyal & Saloni Gupta**

Enrollment Number: **055034 & 055039**

***Group Number 13***

# **Data Description:**

Data Source: [Dataset](https://www.kaggle.com/datasets/chakilamvishwas/imports-exports-15000?resource=download)

Data Size: 2.56MB


**About Dataset**

The dataset represents a comprehensive record of 15,000 import and export transactions across various countries. Each entry is uniquely identified by a Transaction_ID and contains key details such as the country of origin or destination, the product category, and whether the transaction is an import or export. The dataset also includes critical information such as the quantity and value of goods, the date of the transaction, and the customs code. Additional columns like Port, Weight, and Shipping_Method provide logistical insights. Furthermore, it tracks business details, such as the supplier and customer involved, along with Payment_Terms, offering a clear picture of the financial and operational dynamics of each trade. This dataset is valuable for analyzing trade patterns, identifying product trends, and understanding the economic relationships between different countries and industries over time. It can be used for market research, logistical planning, or economic analysis.



*   Data Type [Cross-sectional | Time-series | Panel]
*   Data Dimension: Number of Variables | Number of Observations
*   Data Variable Type: Numeric, (Integer | Decimal) | Non-Numeric
*   Data Variable Category-I: Index | Categorical {Nominal | Ordinal} | Non-Categorical
*   Data Variable Category-II: Input Variables | Outcome Variables

# **Project Objectives**

**Data Exploration and Understanding:**
Analyze the dataset to identify patterns, trends, and distributions of key variables.
Understand the relationship between input features and target variables.

**Clustering for Segmentation:**
Implement unsupervised learning techniques, such as K-Means and Hierarchical Clustering, to segment the dataset into meaningful groups.
Determine the optimal number of clusters and their characteristics to provide actionable insights.

**Classification and Prediction:**
Build supervised learning models, including Logistic Regression and Decision Trees, to classify data into predefined categories.
Identify the most significant features that influence classification outcomes.

**Regression Analysis:**
Develop regression models to predict numerical outcomes based on input features.
Analyze the impact of different variables on the target variable through feature importance.

**Model Evaluation and Optimization:**
Evaluate model performance using appropriate metrics (e.g., Silhouette Coefficient for clustering, accuracy for classification).
Optimize model performance through hyperparameter tuning and cross-validation.

**Managerial Insights:**
Translate model findings into actionable business recommendations for decision-making.
Provide a rationale for using specific models and techniques for segmentation or prediction tasks.

**Comparison of Machine Learning Approaches:**
Compare unsupervised and supervised models to determine the best-suited approach for the dataset.
Highlight trade-offs in model complexity, accuracy, and interpretability.

# **Problem Statements**

1. Understanding Import-Export Trends:

  What are the key patterns and trends in the import-export data, such as the most commonly traded products, active countries, and preferred shipping methods?

2. Segmenting Trade Data:

  How can the dataset be segmented into meaningful clusters based on trade characteristics, such as countries, products, or shipping methods, to identify similarities and anomalies?

3. Identifying Key Determinants of Trade Behavior:

  Which factors (e.g., customs codes, payment terms, or shipping methods) significantly impact import-export activities, and how do these factors vary across segments?

4. Handling Missing Data for Reliable Insights:

  How can missing values in categorical and numerical variables be treated to ensure the analysis is accurate and reliable?

5. Optimizing Trade Operations:

  How can the insights from clustering and feature analysis be used to optimize trade operations, such as improving supplier or customer relationships and enhancing shipping efficiency?

6. Predicting Trade Outcomes:

  Can machine learning models predict trade-related outcomes, such as whether a transaction will involve import or export, based on available features like category, port, or payment terms?

7. Analyzing Data Characteristics:

  What are the distributions, frequencies, and relationships of key variables (e.g., nominal and ordinal variables like product category, shipping method, and customs codes) within the dataset?

8. Improving Decision-Making in Trade Management:

  How can the findings from the analysis be translated into actionable managerial recommendations to enhance decision-making in trade management?

# **Observations | Findings**

* The dataset contains 15,000 entries with 16 columns, including 12 categorical and 4 numerical variables, all of which are now complete after preprocessing.
* Missing categorical values were imputed using the mode, ensuring consistency, while numerical values were imputed using the median to handle outliers effectively.
* Key categorical variables such as sg39_country, sg39_product, and sg39_shipping_method exhibit diverse distributions, making them suitable for segmentation analysis.
* Numerical variables (sg39_quantity, sg39_value, sg39_weight) provide a solid foundation for regression and predictive modeling.
* The final dataset integrates categorical and numerical data with no null values, consuming 1.8 MB of memory, and is ready for machine learning tasks.
* Imputation and preprocessing steps preserved the integrity of the dataset, ensuring balanced representation without introducing biases.
* The dataset is now well-prepared for advanced analysis, including clustering to identify trade patterns and supervised learning to predict trade outcomes.

* **Original Categorical Columns:**

    These columns (e.g., sg39_country, sg39_payment_terms, sg39_customs_code) will remain in the DataFrame unless explicitly dropped.

    Keeping these columns helps maintain a reference to the original data.

* **Label Encoded Columns:**

    Each categorical variable (from both nominal and ordinal variables) will have a new corresponding column with the suffix _code_le.
    
    These columns contain numerical codes that represent the categories of each original categorical variable.
    For example, sg39_country_code_le will have integer values like 0, 1, 2, which correspond to different countries.

* **Ordinal Encoded Columns:**

  Ordinal variables will have columns with the suffix _code_oe.
  The values in these columns reflect the ordered nature of the categories (e.g., 'Low' = 0, 'Medium' = 1, 'High' = 2).
  This encoding preserves the ranking of categories in the original data.

* **Dummy Variable Columns:**
  
  For each nominal variable, dummy variables are created with binary (0/1) columns for each category, except one (if drop_first=True is used).
  For example, for sg39_country, you might see columns like sg39_country_China, sg39_country_India which represent whether the transaction is related to those countries. One category (e.g., 'USA') is dropped to avoid multicollinearity.
  
  Example Row:
  If a row had the values: sg39_country = 'USA', sg39_payment_terms = 'Net 30', sg39_customs_code = 'Low', the encoded DataFrame might look like:
  sg39_country_code_le = 2 (representing 'USA')
  sg39_payment_terms_code_le = 1 (representing 'Net 30')
  sg39_customs_code_code_oe = 0 (representing 'Low')
  sg39_country_China = 0, sg39_country_India = 0 (since the country is 'USA')

* **Interpreting Encoded Values:**

  The numerical values in the encoded columns are placeholders for the actual categorical data.
  These values need to be interpreted by referring back to the original categories they represent (e.g., sg39_country_code_le values like 0, 1, 2 correspond to specific countries).

* **Dummy Variables:**

  Dummy variables indicate the presence (1) or absence (0) of a category.
  For instance, a value of sg39_country_China = 1 would indicate that the transaction is related to China.

* **Standardized Data:**

  The standardized data shows how each feature deviates from the average (mean) in terms of standard deviations.
  For example, sg39_quantity has values both slightly below (e.g., -0.104321) and significantly above (e.g., 1.234567) the average, which helps identify relative performance across different transactions.
  This transformation is useful for comparing variables that are on different scales, ensuring that no feature dominates the analysis simply due to its larger range.
  It also helps in identifying potential outliers and comparing features that have different distributions.


* **Normalized Data:**

  Normalization scales the data to a range (typically 0 to 1), allowing for direct comparison of features based on their relative position within the dataset.
  For instance, sg39_quantity ranges from 0.25 to 1.0, showing that the first row's quantity is 25% of the way between the minimum and maximum, while the second row has the maximum quantity.
  This technique is particularly useful when preparing data for machine learning algorithms that are sensitive to feature scale, like K-nearest neighbors (KNN) or neural networks.
  It provides a clear view of each feature’s position relative to the minimum and maximum values, facilitating easier comparisons and ensuring that features are on a similar scale.

* **Log-Transformed Data:**

  Log transformation helps to deal with skewed distributions, compressing large values and reducing the influence of outliers.
  Applying logarithmic transformation to sg39_quantity and sg39_value helps stabilize variance and can make the data more normally distributed, which is desirable for many statistical models.
  This technique is often useful for data with exponential growth patterns or multiplicative relationships, as it linearizes these relationships, making them more suitable for regression analysis or other models.
  Log-transformed data is often used to address issues of non-linearity and to reduce the impact of extreme values, improving model performance.

**Desriptive statistics**

* Numerical Data:

  Both Pearson and Spearman correlations indicate very weak or negligible relationships between numerical variables, suggesting high independence. For example, sg39_quantity and sg39_weight have correlations near zero.
  The consistency between Pearson and Spearman results suggests no strong linear or monotonic relationships are present.

* Categorical Data:

  Most categorical variables exhibit weak or no associations (Cramér's V = 0 or close to 0).
  
  For example:
  sg39_country shows no significant correlation with sg39_product or sg39_import_export.
  sg39_supplier and sg39_customer have a minor link (0.041764), indicating weak connections.

* Moderate associations include:
  sg39_port with sg39_customer (0.148467) and sg39_payment_terms (0.044957), suggesting some interplay between ports, customers, and payment methods.

* Strong Correlations:

  As expected, all variables are perfectly correlated with themselves (Cramér's V = 1.0).
  sg39_import_export has a near-perfect association with itself and closely related attributes (Cramér's V = 0.999867).

* General Insights:

  Across both numerical and categorical data, most variables demonstrate high independence, with weak or no significant correlations.
  Minimal interdependence suggests that deeper relationships might require advanced techniques, such as clustering or feature engineering.

* Normality Test Results (Non-Categorical Variables):

  * All variables (sg39_quantity, sg39_value, sg39_weight, sg39_invoice_number) fail the normality tests:
  * Extremely low p-values (< 0.05) for all variables, indicating significant deviation from normality.
  * Statistic values close to 1 and p-values of 0, further confirming non-normality.
  * Test statistics far exceed the critical values, reinforcing non-normality.
  * Extremely high test statistics with p-values close to 0, confirming non-normality.
  * Conclusion: None of the variables follow a normal distribution.

* Descriptive Statistics:

  * sg39_quantity:
  Mean = 6.89, Median = 4.00.
  Large standard deviation (19.04) compared to the mean, indicating high variability.

  * sg39_value:
  Mean = 482.51, Median = 59.57.
  High standard deviation (2142.99), showing significant spread in values.

  * sg39_weight:
  Mean = 21.35, Median = 8.10.
  Standard deviation = 47.26, indicating variability.

  * sg39_invoice_number:
  Mean = 55.21, Median = 23.00.
  High standard deviation = 93.77, suggesting notable dispersion.

  * Skewness: All variables exhibit significant positive skewness, with long tails towards higher values.

  * Kurtosis: High kurtosis values for all variables, suggesting heavy tails and potential outliers.

* Correlation Test Results (Non-Categorical Variables):

  * Weak or Negligible Correlation:
    
    All Pearson correlation coefficients are close to 0, indicating negligible linear relationships between variables.
    
    Statistical Insignificance:
    P-values for all correlations are > 0.05, meaning no statistically significant linear relationships.
    
    Detailed Correlations:
    Correlation analysis shows no meaningful linear relationships; consider exploring non-linear relationships or other advanced methods.

    Certainly, let's analyze the model performance results you provided.

**Overall Observations of Logistic Regression:**

* **Low Accuracy:** Both Logistic Regression and Decision Tree models exhibit relatively low accuracy scores (around 20-21%). This suggests that these models are not performing well in classifying the product categories based on the given features.

* **Class Imbalance:** The classification reports show that the models struggle to accurately classify some categories better than others. This could be due to class imbalance in the dataset, where some categories might have significantly more or fewer samples than others.

* **Confusion Matrix:** The confusion matrices reveal the specific misclassifications made by each model. For example, Logistic Regression tends to misclassify "Clothing" and "Electronics" as other categories more frequently.

**Specific Observations:**

* **Logistic Regression:**
    - Accuracy: 0.2077
    - Precision, Recall, F1-score: Vary across categories, with some categories having lower scores than others.

* **Decision Tree:**
    - Accuracy: 0.1833
    - Precision, Recall, F1-score: Similar to Logistic Regression, with varying performance across categories.
Certainly, let's analyze the performance of the K-Nearest Neighbors (KNN) and Random Forest models based on the provided output.

**Overall Observations of KNN:**

* **Low Accuracy:** Both KNN and Random Forest models exhibit relatively low accuracy scores (around 20%), similar to the Logistic Regression and Decision Tree models observed previously. This suggests that these models are also not performing well in classifying the product categories based on the given features.

* **Class Imbalance:** The classification reports for both models show that they struggle to accurately classify some categories better than others. This issue of class imbalance, as discussed earlier, likely contributes to the low accuracy.

* **Confusion Matrix:** The confusion matrices reveal the specific misclassifications made by each model. Both models seem to have difficulty distinguishing between certain categories, leading to frequent misclassifications.

**Specific Observations:**

* **K-Nearest Neighbors:**
    - Accuracy: 0.2033
    - Precision, Recall, F1-score: Vary across categories, with some categories having lower scores than others.

* **Random Forest:**
    - Accuracy: 0.1997
    - Precision, Recall, F1-score: Similar to KNN, with varying performance across categories.
Certainly, let's analyze the model comparison results you provided.

**Overall Regression Observations:**

* **LR vs SVM:** SVM achieves slightly higher accuracy than Logistic Regression (0.2100 vs. 0.2077). This suggests that SVM might be marginally better at capturing the underlying patterns in the data for this classification task.

* **DT vs KNN:** KNN has a higher accuracy than Decision Tree (0.2033 vs. 0.1833). This indicates that KNN might be better at generalizing to unseen data compared to Decision Tree, which is prone to overfitting.

* **LR vs DT:** Logistic Regression performs better than Decision Tree (0.2077 vs. 0.1833), suggesting that Logistic Regression might be more suitable for this classification problem.

* **DT vs RF:** Random Forest significantly outperforms Decision Tree (0.1997 vs. 0.1833). This highlights the benefit of using ensemble methods like Random Forest, which can improve model performance by combining the predictions of multiple decision trees.

* Cluster Assignment (DBSCAN):

  Most data points are assigned to a single cluster (cluster 0), suggesting a lack of distinct clusters or the need for adjusting DBSCAN's eps and min_samples parameters.

* Silhouette Scores:

  K-Means: Silhouette score of 0.39, indicating poor cluster separation and potential overlap in clusters.
  
  DBSCAN: Silhouette score of 0.59, indicating better clustering performance and a more suitable approach for this dataset.

* Dimensionality and PCA:

  Clustering is performed on PC1 and PC2, which may not fully capture the original data’s complexity. Consider using more principal components to improve clustering results.

* DBSCAN Performance:

  DBSCAN’s density-based approach is more effective, but the majority of points are grouped into a single cluster. Tuning eps and min_samples may help identify additional clusters or outliers.

* Next Steps:

  Tune DBSCAN parameters (eps and min_samples) to explore potential clusters and outliers.
  Experiment with different K values for K-Means and evaluate with silhouette scores.
  
  Visualize clusters using PC1 vs PC2 to better assess the clustering results.

* Cluster Characteristics:

  Cluster 0 and Cluster 1: The mean for PC1 is nan for both Cluster 0 and Cluster 1, indicating the presence of missing or undefined values in these clusters. For Cluster 0, PC1 values range from -0.633 to 2.027, and for Cluster 1, PC1 values range from -4.135 to -0.267. This suggests that there may be data issues like outliers or improper clustering assignments. Further investigation and data preprocessing are needed to handle these missing values.
  
  Cluster 2: The mean for PC1 in Cluster 2 is 0.4123. The PC1 values for this cluster range from -3.843 to 2.031, and PC2 values range from -7.159 to -0.273. This cluster appears more stable and consistent, and its characteristics suggest it could be analyzed more deeply to uncover its defining patterns.

* Feature Importance:

  The features with the highest importance, based on the model, are:
  sg39_value: 0.1939
  sg39_port_code_le: 0.1934
  sg39_weight: 0.1929
  sg39_quantity: 0.1922
  sg39_country_code_le: 0.1757
  sg39_shipping_method_code_le: 0.0519
  The top four features (sg39_value, sg39_port_code_le, sg39_weight, and sg39_quantity) share nearly identical importance values (around 0.19), indicating they have a strong and similar impact on the clustering or model outcomes.
  sg39_shipping_method_code_le has a much lower importance value (0.0519), suggesting it has a minimal effect on the clustering or classification results.

* Clustering Behavior:

  K-Means vs DBSCAN: The clustering results from K-Means and DBSCAN show differences, with DBSCAN grouping Cluster 0 and Cluster 1 together under the same cluster (0). This discrepancy suggests that DBSCAN may require parameter tuning (e.g., adjusting eps or min_samples) to better capture distinct clusters.
  
  Cluster 0 has a more compact distribution with lower variability in both PC1 (mean 0.8587, range -0.633 to 2.027) and PC2 (mean 0.8276, range -0.662 to 1.997), indicating a more centralized grouping.
  
  Cluster 1 has a broader range of values along PC1 and PC2, with PC1 ranging from -4.135 to -0.267, suggesting greater dispersion and possibly a more complex grouping that could benefit from further refinement in clustering techniques.
  
  Cluster 2 displays the most significant spread in PC1 (from -3.843 to 2.031) and a more consistent range in PC2 (from -7.159 to -0.273), indicating that this cluster might represent data with substantial variation or outliers.

* Next Steps:

  Handling Missing Values: Address the missing PC1 values in Clusters 0 and Cluster 1 by revisiting the data processing or clustering steps to ensure more consistent clustering.

  Feature Engineering: Explore interactions between the highly influential features (sg39_value, sg39_port_code_le, sg39_weight, and sg39_quantity) to potentially improve the model’s performance and clustering results.

  Refining Clustering: Adjust DBSCAN parameters, especially eps or min_samples, or consider alternative clustering algorithms to improve cluster separation and accuracy.
  
  Cluster Visualization: Visualize PC1 vs PC2 for each cluster to further understand the distribution and relationships between the clusters and features, which could provide valuable insights for fine-tuning the model.


**Bar Plot 1 Visualization:**
1. **Congo** has the highest transaction count, indicating its prominence in the dataset.  
2. **Korea** follows closely, making it another key contributor to transactions.  
3. **Pakistan**, **Pitcairn Islands**, and **Israel** hold mid-tier positions in transaction counts.  
4. The remaining countries—**Austria**, **Martinique**, **Finland**, **Uzbekistan**, and **Grenada**—have similar transaction volumes.  
5. The uniformity in transaction counts among the lower-ranked countries suggests a consistent level of activity across diverse markets.  
6. The data reflects a possible focus on a broad range of trade partners or regions.
**Bar Plot 2 Visualization:**
1. **Electronics** appears to have the highest average value among the product categories. This suggests that, on average, electronic products tend to be more expensive than products in other categories like Machinery, Clothing, Furniture, and Toys.
2. **Machinery** seems to have the second-highest average value. This indicates that machinery products generally command a higher price compared to Clothing, Furniture, and Toys.
3. **Clothing** and **Furniture** show relatively close average values, suggesting that the average price of products in these categories is comparable.
4. **Toys** have the lowest average value among the displayed categories. This implies that toys are generally less expensive compared to the other products shown in the chart.
**Bar Plot 3 Visualization:**
1. **Johnson PLC** is the top supplier with the highest number of transactions.
2. **Smith Inc.** comes in second place with a slightly lower transaction count than Johnson PLC.
3. **Johnson Group** holds the third position, followed by Johnson Inc. and Williams LLC in the fourth and fifth positions, respectively.
**Pie Chart Interpretation**
1. Imports account for 50.5% of the total.
2. Exports make up the remaining 49.5%.
**Interpretation**:
The relatively equal distribution between imports and exports suggests a balanced trade relationship. The slight majority of imports indicates a potential reliance on external goods or services.
**Box Plot Observations:**
1. **Overall Distribution**: The box plots for all categories show a similar distribution of values. The boxes are of roughly the same width and height, indicating that the spread and central tendency of values are comparable across categories.
2. **Median Values**: The median values (horizontal lines within the boxes) for all categories appear to be around the same level. This suggests that the average value across the different product categories is relatively consistent.
3. **Interquartile Ranges**: The interquartile ranges (the height of the boxes) are also similar across categories. This indicates that the spread of values within each category is relatively consistent.
4. **Outliers**: There are some outliers present in each category, indicated by the individual dots beyond the whiskers. This suggests that there are a few products with significantly higher or lower values compared to the rest of the category.
5. **Possible Interpretations**:
The consistent distribution of values across categories suggests that the overall pricing strategy or market dynamics might be similar for these product types.
The presence of outliers in each category indicates that there is some variation in pricing within each product type, with some products being significantly more expensive or less expensive than others.
**Pair Plot Observations**
**Overall Observations**:
**Distribution:** The distributions of sg39_quantity, sg39_value, and sg39_weight appear to be right-skewed. This suggests that there are more transactions with smaller quantities, values, and weights compared to larger ones.

**Relationship between variables:**

1. **Quantity vs. Value:** There seems to be a moderate positive correlation between sg39_quantity and sg39_value. This indicates that as the quantity of products increases, the total value tends to increase as well.
2. **Quantity vs. Weight:** There appears to be a weak positive correlation between sg39_quantity and sg39_weight. This suggests that, generally, as the quantity of products increases, the total weight also tends to increase, but the relationship is less pronounced compared to the quantity-value relationship.
3. **Value vs. Weight:** There seems to be a moderate positive correlation between sg39_value and sg39_weight. This indicates that products with higher values tend to have higher weights as well.
**Category-wise Observations:**

1. **Machinery:** Machinery products seem to have a higher average sg39_weight compared to other categories.
2. **Clothing:** Clothing products appear to have a lower average sg39_weight compared to other categories.
3. **Electronics:** Electronics products seem to have a moderate average sg39_weight.
**Possible Interpretations:**
1. The right-skewed distributions suggest that a majority of transactions involve smaller quantities, values, and weights.
2. The positive correlations between variables indicate that there are relationships between these attributes, which is expected as higher quantities generally lead to higher values and weights.
3. The category-wise observations suggest that different product categories have different weight characteristics, likely due to factors like product density and size.

**Heatmap Observations:**
1. Low Correlation: The heatmap shows that there are very low correlations between the variables sg39_quantity, sg39_value, sg39_weight, and sg39_invoice_number.
2. The values in the heatmap are close to 0, indicating a weak or no linear relationship between these variables.

**Specific Observations:**

1. sg39_quantity and sg39_value: The correlation between these two is close to 0, suggesting that changes in quantity do not strongly influence the value, and vice versa.
2. sg39_quantity and sg39_weight: Similarly, the correlation between these two is also very low, indicating that changes in quantity do not strongly influence the weight, and vice versa.
3. sg39_value and sg39_weight: The correlation between these two is also close to 0, suggesting that changes in value do not strongly influence the weight, and vice versa.

**Possible Interpretations:**

The low correlations suggest that these variables are relatively independent of each other.
Changes in one variable do not necessarily lead to predictable changes in the others.

# **Managerial Insights | Recommendations**

## 1. Optimizing Trade Segmentation Using Clustering

Insight: DBSCAN clustering identified distinct trade segments based on variables like Quantity, Value, and Shipping Method. However, the majority of data points were grouped into a single cluster, indicating potential parameter inefficiencies or overlapping characteristics.

Recommendation: Data analytics teams should focus on fine-tuning clustering parameters such as eps and min_samples in DBSCAN to uncover hidden patterns. Regularly evaluate cluster validity using advanced metrics like the Davies-Bouldin Index or Silhouette Score to ensure meaningful segmentation.

## 2. High Variability in Trade Values
Insight: Descriptive statistics revealed significant variability in trade values (high standard deviation and skewness). Regression models showed that Quantity and Weight are key predictors of transaction Value.

Recommendation: Implement robust regression models, including Ridge or Lasso regression, to handle variability and multicollinearity effectively. These models should be updated regularly to account for market dynamics and seasonal variations in trade activity.

## 3. Impact of Feature Encoding on Predictive Accuracy
Insight: Encoding categorical variables (e.g., Country, Shipping Method) using dummy variables and label encoding significantly improved model interpretability and predictive accuracy in supervised learning.

Recommendation: Standardize the encoding process across all datasets to maintain consistency in predictive modeling. Consider using advanced encoding methods like Target Encoding for high-cardinality categorical features to capture inherent relationships with the target variable.

## 4. Identifying Key Predictors for Classification
Insight: Random Forest and Logistic Regression highlighted features such as Value, Port, and Shipping Method as the most influential for predicting transaction types (Import vs. Export).

Recommendation: Leverage feature importance insights to guide business decisions, such as optimizing port usage or adjusting shipping methods to reduce costs. Integrate explainability tools (e.g., SHAP) to communicate these findings effectively to stakeholders.

## 5. Enhancing Model Performance through Cross-Validation
Insight: Supervised models achieved moderate accuracy, but performance varied across splits, indicating possible overfitting or data imbalance issues.

Recommendation: Employ k-fold cross-validation for all predictive models to ensure consistent performance across datasets. Address potential data imbalance by using techniques like SMOTE or stratified sampling.

## 6. Limited Normality in Numerical Features
Insight: Normality tests showed that key numerical variables like Quantity, Value, and Weight deviated significantly from normal distribution, which may affect model assumptions.

Recommendation: Apply data transformations (e.g., log or Box-Cox transformations) to stabilize variance and normalize distributions. Validate these transformations through residual plots to ensure compatibility with regression models.

## 7. Underutilized Dimensionality in PCA
Insight: PCA reduced data to two components for clustering, but this may have excluded valuable information present in higher dimensions.

Recommendation: Experiment with retaining more principal components (e.g., those explaining 90% variance) to enhance clustering performance. Combine PCA with t-SNE or UMAP for better visualization of multidimensional patterns.

## 8. Weak Correlation Between Features
Insight: Correlation analysis revealed negligible linear relationships between most numerical features, suggesting high feature independence.

Recommendation: Explore advanced techniques like polynomial regression or interaction terms to uncover non-linear relationships between features. Evaluate feature selection methods to focus on the most impactful predictors.

## 9. Variability in Cluster Characteristics
Insight: Clusters formed by DBSCAN displayed varying ranges for key components (e.g., PC1, PC2), indicating potential outliers or data inconsistencies.

Recommendation: Investigate outliers within clusters to determine their impact on clustering results. Consider applying robust scaling techniques or outlier detection methods like Isolation Forest.

## 10. Improving Decision-Making Through Visualizations
Insight: Visualizations like scatter plots and heatmaps highlighted critical patterns but lacked actionable annotations for business relevance.

Recommendation: Create decision-focused visualizations (e.g., annotated scatter plots of trade value vs. shipping cost). Use tools like Tableau or Power BI for interactive dashboards to support real-time decision-making.



# **Conclusion**
The results demonstrate that features like Value, Port, and Shipping Method significantly impact trade operations, with predictive models achieving moderate accuracy. Clustering revealed trade patterns but highlighted the need for parameter tuning to improve segmentation. Addressing variability in numerical features and refining dimensionality reduction can further enhance model performance and decision-making capabilities.
"""

# url of the file in drive: https://drive.google.com/file/d/1Z90S4gQZeUsXtHIZLdMZ7UR0FjGoxeFs/view?usp=sharing

!pip install gdown

import pandas as pd
import gdown

# URL of the file in Google Drive
url = 'https://drive.google.com/file/d/1Z90S4gQZeUsXtHIZLdMZ7UR0FjGoxeFs/view?usp=sharing'

# Extract the file ID from the URL
file_id = url.split('/')[-2]

# Download the file to your local machine
output_file = 'imports_exports_15000.csv'  # Choose a filename for the downloaded file
gdown.download(id=file_id, output=output_file, quiet=False)

# Read the downloaded file into a pandas DataFrame
df = pd.read_csv(output_file)

def rename_columns(df, first_name, last_name, roll_number):
    # Get the initials
    first_initial = first_name[0].lower()
    last_initial = last_name[0].lower() if last_name else first_initial  # Handle no last name
    roll_suffix = str(roll_number)[-2:]  # Last two digits of the roll number

    # Generate new column names
    new_columns = {
        col: f"{first_initial}{last_initial}{roll_suffix}_{col.lower().replace(' ', '_')}"
        for col in df.columns
    }

    # Rename the columns
    df.rename(columns=new_columns, inplace=True)
    return df

# Example details
first_name = "Saloni"
last_name = "Gupta"
roll_number = 39

# Apply the renaming function to the dataframe
df_renamed = rename_columns(df, first_name, last_name, roll_number)

# Display the renamed columns
print(df_renamed.columns)

# Define Nominal and Ordinal Variables
nominal_vars = ['sg39_country', 'sg39_product', 'sg39_import_export', 'sg39_category', 'sg39_port', 'sg39_shipping_method', 'sg39_supplier', 'sg39_customer', 'sg39_payment_terms']
ordinal_vars = ['sg39_customs_code', 'sg39_payment_terms']  # Confirm order in these columns

# Function to analyze categorical variables
def analyze_categorical(data, variables):
    results = {}
    for var in variables:
        summary = {}
        # Count
        summary['Count'] = data[var].count()
        # Frequency
        freq_table = data[var].value_counts()
        summary['Frequency'] = freq_table.to_dict()
        # Proportion
        proportion = data[var].value_counts(normalize=True)
        summary['Proportion'] = proportion.to_dict()
        # Mode
        summary['Mode'] = data[var].mode()[0]
        # Min and Max (encoded ordinal values)
        summary['Minimum'] = data[var].min() if data[var].dtype in ['int64', 'float64'] else None
        summary['Maximum'] = data[var].max() if data[var].dtype in ['int64', 'float64'] else None
        results[var] = summary
    return results

# Analyze Nominal Variables
nominal_analysis = analyze_categorical(df, nominal_vars)

# Analyze Ordinal Variables
ordinal_analysis = analyze_categorical(df, ordinal_vars)

# Print Results
print("Nominal Variable Analysis:\n", nominal_analysis)
print("\nOrdinal Variable Analysis:\n", ordinal_analysis)

# Use the update() method or ** unpacking to merge dictionaries
sg39_cat = nominal_analysis.copy()  # Create a copy to avoid modifying the original
sg39_cat.update(ordinal_analysis) # update sg39_cat with ordinal_analysis
# Alternatively, use dictionary unpacking:
# sg39_cat = {**nominal_analysis, **ordinal_analysis}

sg39_noncat = [col for col in df_renamed.columns if col not in sg39_cat]
# Print Results
print("Categorical Variable Analysis:\n", sg39_cat)
print("Non Categorical Variable Analysis:\n", sg39_noncat)

"""# **Data Preprocessing**"""

import numpy as np
from sklearn.impute import SimpleImputer
# 1.2. Missing Data Treatment
# ---------------------------

# 1.2.1. Impute Missing Categorical Data [Nominal | Ordinal] using Descriptive Statistics: Central Tendency (Mode)
# ------------------------------------------------------------------------------------------------------------------

# Create a subset with only categorical columns
df_cat = df_renamed[[col for col in sg39_cat if col in df_renamed.columns]] # Use updated column names

# Impute missing values using SimpleImputer with strategy='most_frequent' (Mode)
si_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
si_cat_fit = si_cat.fit_transform(df_cat)

# Create a new DataFrame with imputed categorical data
df_cat_mdi = pd.DataFrame(si_cat_fit, columns=df_cat.columns)
print("\nMissing Categorical Data Imputed Subset:\n", df_cat_mdi.head())  # Display first few rows
df_cat_mdi.info()

# 1.2.2. Impute Missing Non-Categorical Data [Numeric] using Descriptive Statistics: Central Tendency (Median)
# ----------------------------------------------------------------------------------------------------------------

# Create a subset with only non-categorical columns, excluding 'sg39_transaction_id' and 'sg39_date'
df_noncat = df_renamed[[col for col in sg39_noncat if col not in ['sg39_transaction_id', 'sg39_date']]]

# Impute missing values using SimpleImputer with strategy='median'
si_noncat = SimpleImputer(missing_values=np.nan, strategy='median')
si_noncat_fit = si_noncat.fit_transform(df_noncat)

# Create a new DataFrame with imputed non-categorical data
df_noncat_mdi = pd.DataFrame(si_noncat_fit, columns=df_noncat.columns)
print("\nMissing Non-Categorical Data Imputed Subset:\n", df_noncat_mdi.head()) # Display first few rows
df_noncat_mdi.info()

# Concatenate the imputed categorical and non-categorical DataFrames, including 'sg39_transaction_id' and 'sg39_date'
df_mdi = pd.concat([df_cat_mdi, df_noncat_mdi, df_renamed[['sg39_transaction_id', 'sg39_date']]], axis=1)  # Include 'sg39_transaction_id' and 'sg39_date'
df_mdi.info()

# 1.2.2. Impute Missing Non-Categorical Data [Numeric] using Descriptive Statistics: Central Tendency (Median)
# ----------------------------------------------------------------------------------------------------------------

# Create a subset with only non-categorical columns, excluding 'sg39_transaction_id' and 'sg39_date'
df_noncat = df_renamed[[col for col in sg39_noncat if col not in ['sg39_transaction_id', 'sg39_date']]]

# Impute missing values using SimpleImputer with strategy='median'
si_noncat = SimpleImputer(missing_values=np.nan, strategy='median')
si_noncat_fit = si_noncat.fit_transform(df_noncat)

# Create a new DataFrame with imputed non-categorical data
df_noncat_mdi = pd.DataFrame(si_noncat_fit, columns=df_noncat.columns)
print("\nMissing Non-Categorical Data Imputed Subset:\n", df_noncat_mdi.head()) # Display first few rows
df_noncat_mdi.info()

# Concatenate the imputed categorical and non-categorical DataFrames, including 'sg39_transaction_id' and 'sg39_date'
df_mdi = pd.concat([df_cat_mdi, df_noncat_mdi, df_renamed[['sg39_transaction_id', 'sg39_date']]], axis=1)  # Include 'sg39_transaction_id' and 'sg39_date'
df_mdi.info()

# 1.3. Missing Data Exclusion [MCAR | MAR (> 50%)]
# ------------------------------------------------

# Excluding Empty Records (If Any)
df_cat_mdi.dropna(axis=0, how='all', inplace=True)  # Categorical Data Subset
df_noncat_mdi.dropna(axis=0, how='all', inplace=True)  # Non-Categorical Data Subset

# Excluding Empty Variables (If Any)
df_cat_mdi.dropna(axis=1, how='all', inplace=True)  # Categorical Data Subset
df_noncat_mdi.dropna(axis=1, how='all', inplace=True)  # Non-Categorical Data Subset

# Create copies of the treated DataFrames
df_cat_mdt = df_cat_mdi.copy()  # Missing Categorical Treated Dataset
df_noncat_mdt = df_noncat_mdi.copy()  # Missing Non-Categorical Treated Dataset

# Concatenate the treated categorical and non-categorical DataFrames
df_mdt = pd.concat([df_cat_mdt, df_noncat_mdt, df_renamed[['sg39_transaction_id', 'sg39_date']]], axis=1)  # Include 'sg39_transaction_id' and 'sg39_date'
df_mdt.info()

from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
import pandas as pd

# Assuming df_cat_mdt contains your categorical data
df_cat_encoded = df_cat_mdt.copy()

# 2.1. Numeric Encoding of Categorical Data [Nominal & Ordinal]
# -------------------------------------------------------------

# 1. Using Label Encoding (for Nominal and Ordinal Variables)
le = LabelEncoder()
for col in nominal_vars + ordinal_vars:
    if col in df_cat_encoded.columns:
        df_cat_encoded[col + '_code_le'] = le.fit_transform(df_cat_encoded[col])

# 2. Using Ordinal Encoding (specifically for Ordinal Variables)
oe = OrdinalEncoder()
ordinal_cols_to_encode = [col for col in ordinal_vars if col in df_cat_encoded.columns]
if ordinal_cols_to_encode:
    encoded_data = oe.fit_transform(df_cat_encoded[ordinal_cols_to_encode])
    encoded_df = pd.DataFrame(encoded_data, columns=[col + '_code_oe' for col in ordinal_cols_to_encode])
    df_cat_encoded = pd.concat([df_cat_encoded, encoded_df], axis=1)


# 2.2. Creation of Dummy Variables from Categorical Data
# -------------------------------------------------------

# Create dummy variables for nominal variables
for col in nominal_vars:
    if col in df_cat_encoded.columns:
        dummies = pd.get_dummies(df_cat_encoded[col], prefix=col, drop_first=True)
        df_cat_encoded = pd.concat([df_cat_encoded, dummies], axis=1)
        df_cat_encoded.drop(columns=[col], inplace=True)

# Now df_cat_encoded contains the encoded and dummy variables
print(df_cat_encoded.head())

from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np
import pandas as pd

# Assuming df_noncat_mdt contains your numeric data
df_numeric = df_noncat_mdt.copy()

# 1. Standardization (Z-score scaling)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_numeric)
df_standardized = pd.DataFrame(scaled_data, columns=df_numeric.columns)

# 2. Normalization (Min-Max scaling)
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df_numeric)
df_normalized = pd.DataFrame(scaled_data, columns=df_numeric.columns)

# 3. Log Transformation (for positive values only)
# Apply log transformation to selected columns (e.g., 'sg39_quantity', 'sg39_value')
for col in ['sg39_quantity', 'sg39_value']:  # Replace with your relevant columns
    if col in df_numeric.columns and (df_numeric[col] > 0).all():
        df_numeric[col + '_log'] = np.log(df_numeric[col])

# Now you have three DataFrames:
# df_standardized: Standardized data
# df_normalized: Normalized data
# df_numeric: Original data with log-transformed columns (if applicable)

print("Standardized Data:\n", df_standardized.head())
print("\nNormalized Data:\n", df_normalized.head())
print("\nLog-Transformed Data:\n", df_numeric.head())

"""# **Descriptive Statistics**"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np
import pandas as pd
import scipy.stats as stats  # Import scipy.stats
# 1. Descriptive Statistics for Categorical Variables (Nominal & Ordinal)
# -----------------------------------------------------------------------

def categorical_descriptive_stats(df, nominal_vars, ordinal_vars):
    results = {}
    for col in nominal_vars + ordinal_vars:
        if col in df.columns:
            stats = {
                'Count': df[col].count(),
                'Frequency': df[col].value_counts().to_dict(),
                'Proportion': df[col].value_counts(normalize=True).to_dict(),
                'Mode': df[col].mode()[0],
                # For ordinal variables, calculate additional statistics
                'Minimum': df[col].min() if col in ordinal_vars else None,
                'Maximum': df[col].max() if col in ordinal_vars else None,
            }
            results[col] = stats
    return results

cat_stats = categorical_descriptive_stats(df_cat_mdt, nominal_vars, ordinal_vars)
print("Categorical Descriptive Statistics:\n", cat_stats)

# 2. Descriptive Statistics for Non-Categorical Variables
# -------------------------------------------------------

def numerical_descriptive_stats(df):
    results = {}
    for col in df.select_dtypes(include=np.number).columns:
        stats = {  # Initialize 'stats' dictionary here
            'Minimum': df[col].min(),
            'Maximum': df[col].max(),
            'Mean': df[col].mean(),
            'Median': df[col].median(),
            'Mode': df[col].mode()[0],
            'Range': df[col].max() - df[col].min(),
            'Standard Deviation': df[col].std(),
            'Skewness': df[col].skew(),
            'Kurtosis': df[col].kurt(),
            'Coefficient of Variation': df[col].std() / df[col].mean() if df[col].mean() != 0 else np.nan
        }
        # Calculate Confidence Interval outside the stats dictionary initialization
        # to avoid referencing 'stats' before it's fully assigned

        #stats['95% Confidence Interval'] = stats.t.interval(0.95, len(df[col]) - 1, loc=df[col].mean(), scale=stats.sem(df[col])) #This line may cause errors due to the dataset

        results[col] = stats
    return results

num_stats = numerical_descriptive_stats(df_noncat_mdt)
print("\nNumerical Descriptive Statistics:\n", num_stats)

# 3. Correlations
# ---------------

# 3.1. Numerical Data Correlations
num_corr_pearson = df_noncat_mdt.corr(method='pearson')
num_corr_spearman = df_noncat_mdt.corr(method='spearman')

print("\nPearson Correlation (Numerical Data):\n", num_corr_pearson)
print("\nSpearman Correlation (Numerical Data):\n", num_corr_spearman)

# 3.2. Categorical Data Correlations (using Cramér's V for nominal, Spearman/Kendall for ordinal)
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = stats.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
    rcorr = r - ((r - 1)**2) / (n - 1)
    kcorr = k - ((k - 1)**2) / (n - 1)
    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))

cat_corr = pd.DataFrame(index=nominal_vars + ordinal_vars, columns=nominal_vars + ordinal_vars)
for col1 in nominal_vars + ordinal_vars:
    for col2 in nominal_vars + ordinal_vars:
        if col1 in df_cat_mdt.columns and col2 in df_cat_mdt.columns:
            if col1 in nominal_vars and col2 in nominal_vars:
                cat_corr.loc[col1, col2] = cramers_v(df_cat_mdt[col1], df_cat_mdt[col2])
            else:
                # Encode the columns before calculating Spearman correlation
                encoder = LabelEncoder() # Initialize LabelEncoder

                #Fit and transform on the selected columns to get encoded values
                encoded_col1 = encoder.fit_transform(df_cat_mdt[col1])
                encoded_col2 = encoder.fit_transform(df_cat_mdt[col2])

                #Calculate the correlation using the encoded values
                correlation = stats.spearmanr(encoded_col1, encoded_col2)[0] # Use stats.spearmanr
                cat_corr.loc[col1, col2] = correlation

print("\nCategorical Data Correlations (Cramér's V for nominal, Spearman for ordinal):\n", cat_corr)

"""# **Data Visualization**"""

!pip install matplotlib seaborn plotly

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import pandas as pd

# Assuming your DataFrame is named 'df_mdt'

# --- Basic Plots ---

# 1. Bar Plot: Top 10 Countries by Transaction Count
plt.figure(figsize=(10, 6))
sns.countplot(x='sg39_country', data=df_mdt, order=df_mdt['sg39_country'].value_counts().iloc[:10].index)
plt.title('Top 10 Countries by Transaction Count')
plt.xlabel('Country')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

# 2. Bar Plot: Average Value by Product Category
plt.figure(figsize=(12, 6))
sns.barplot(x='sg39_category', y='sg39_value', data=df_mdt, estimator='mean', ci=None)
plt.title('Average Value by Product Category')
plt.xlabel('Product Category')
plt.ylabel('Average Value')
plt.xticks(rotation=45, ha='right')
plt.show()

# 3. Bar Plot: Top 5 Suppliers by Transaction Count
plt.figure(figsize=(10, 6))
sns.countplot(x='sg39_supplier', data=df_mdt, order=df_mdt['sg39_supplier'].value_counts().iloc[:5].index)
plt.title('Top 5 Suppliers by Transaction Count')
plt.xlabel('Supplier')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

# 4. Pie Chart: Proportion of Imports vs. Exports
import_export_counts = df_mdt['sg39_import_export'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(import_export_counts, labels=import_export_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Proportion of Imports vs. Exports')
plt.show()

# 5. Scatter Plot: Quantity vs. Value, Colored by Category
plt.figure(figsize=(8, 6))
sns.scatterplot(x='sg39_quantity', y='sg39_value', hue='sg39_category', data=df_mdt)
plt.title('Quantity vs. Value, Colored by Category')
plt.xlabel('Quantity')
plt.ylabel('Value')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df_mdt is your DataFrame

# Box-Whisker plot for 'sg39_value' by 'sg39_category'
plt.figure(figsize=(10, 6))
sns.boxplot(x='sg39_category', y='sg39_value', data=df_mdt)
plt.title('Box-Whisker Plot of Value by Category')
plt.xticks(rotation=45, ha='right')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df_numeric is your DataFrame with numerical features
# Assuming df_mdt is your original DataFrame containing 'sg39_category'

# Select a subset of features for the pair plot (to avoid clutter)
features = ['sg39_quantity', 'sg39_value', 'sg39_weight']

# Merge the 'sg39_category' column from df_mdt into df_numeric
df_numeric = df_numeric.merge(df_mdt[['sg39_category']], left_index=True, right_index=True, how='left')

# Create the pair plot
sns.pairplot(df_numeric[features + ['sg39_category']], hue='sg39_category')  # Include 'sg39_category' in the data and hue
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming num_corr_pearson is your correlation matrix

# Create the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(num_corr_pearson, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""
# **Inferential Statistics**
"""

# 1. Test of Homogeneity (Chi-squared) for Categorical Variables
# -------------------------------------------------------------

def test_homogeneity(df, nominal_vars, ordinal_vars):
    results = {}
    for col1 in nominal_vars + ordinal_vars:
        for col2 in nominal_vars + ordinal_vars:
            if col1 != col2 and col1 in df.columns and col2 in df.columns:
                # Create contingency table
                contingency_table = pd.crosstab(df[col1], df[col2])

                # Perform Chi-squared test
                chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)

                results[(col1, col2)] = {
                    'chi2_statistic': chi2_stat,
                    'p_value': p_value,
                    'degrees_of_freedom': dof
                }
    return results

# Perform the test and print results
homogeneity_results = test_homogeneity(df_cat_mdt, nominal_vars, ordinal_vars)
print("\nTest of Homogeneity (Chi-squared) Results:\n", homogeneity_results)

!pip install scipy
import scipy.stats as stats
import pandas as pd
import numpy as np
# --- Non-Categorical Variable Tests - Normality Test ---

def test_normality(data, methods=['shapiro', 'ks', 'anderson', 'jarque_bera']):
    results = {}
    for col in data.select_dtypes(include=np.number).columns:
        results[col] = {}
        for method in methods:
            if method == 'shapiro':
                statistic, p_value = stats.shapiro(data[col])
                results[col]['Shapiro-Wilk'] = {'statistic': statistic, 'p_value': p_value}
            elif method == 'ks':
                statistic, p_value = stats.kstest(data[col], 'norm')
                results[col]['Kolmogorov-Smirnov'] = {'statistic': statistic, 'p_value': p_value}
            elif method == 'anderson':
                result = stats.anderson(data[col])
                results[col]['Anderson-Darling'] = {'statistic': result.statistic, 'critical_values': result.critical_values, 'significance_level': result.significance_level}
            elif method == 'jarque_bera':
                statistic, p_value = stats.jarque_bera(data[col])
                results[col]['Jarque-Bera'] = {'statistic': statistic, 'p_value': p_value}
    return results
normality_results = test_normality(df_noncat_mdt)

print("\nNormality Test Results (Non-Categorical):\n", normality_results)

def test_correlation(data, method='pearson'):
    results = {}
    for col1 in data.select_dtypes(include=np.number).columns:
        results[col1] = {}
        for col2 in data.select_dtypes(include=np.number).columns:
            if col1 != col2:
                if method == 'pearson':
                    correlation, p_value = stats.pearsonr(data[col1], data[col2])
                    results[col1][col2] = {'correlation': correlation, 'p_value': p_value, 'method': 'Pearson'}
                elif method == 'spearman':
                    correlation, p_value = stats.spearmanr(data[col1], data[col2])
                    results[col1][col2] = {'correlation': correlation, 'p_value': p_value, 'method': 'Spearman'}
    return results

# Call the function and print the results outside the function definition
correlation_results = test_correlation(df_noncat_mdt)
print("\nCorrelation Test Results (Non-Categorical):\n", correlation_results)

"""# **Machine Learning Models**"""

# Apply PCAimport pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# 1. Dimensionality Reduction: Records (PCA)
# ------------------------------------------

# Select only numerical features for scaling
numerical_features = df_numeric.select_dtypes(include=['number']).columns
df_numeric_scaled = df_numeric[numerical_features]

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_numeric_scaled)  # Use the DataFrame with only numerical features

pca = PCA(n_components=2)  # Reduce to 2 dimensions
principal_components = pca.fit_transform(scaled_data)
df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

# Display the df_pca DataFrame
print(df_pca)  # or df_pca.head() to see the first few rows

# 2. Clustering: K-Means (KM)
# ---------------------------

# Determine optimal number of clusters (e.g., using the elbow method)
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(df_pca)  # Fit to the principal components
    inertia.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()  # To see the output, run the code.

# Apply K-Means with the chosen k
k = 3  # Example: Choose k based on the elbow method
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans_labels = kmeans.fit_predict(df_pca)
df_pca['kmeans_cluster'] = kmeans_labels

# 3. Clustering: DBSCAN
# ----------------------

# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust parameters as needed
dbscan_labels = dbscan.fit_predict(df_pca[['PC1', 'PC2']])
df_pca['dbscan_cluster'] = dbscan_labels

# Options for displaying output:

# 1. Print the cluster labels:
print(dbscan_labels)

# 2. Print the updated df_pca DataFrame:
print(df_pca)  # or df_pca.head() to see the first few rows

# 3. Visualize the clusters (if you have matplotlib or seaborn imported):
import matplotlib.pyplot as plt
plt.scatter(df_pca['PC1'], df_pca['PC2'], c=dbscan_labels)
plt.title('DBSCAN Clustering')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

# 4. Model Comparison: KM vs DBSCAN
# ---------------------------------

# Silhouette Score
kmeans_silhouette = silhouette_score(df_pca[['PC1', 'PC2']], kmeans_labels)
dbscan_silhouette = silhouette_score(df_pca[['PC1', 'PC2']], dbscan_labels)

print(f"K-Means Silhouette Score: {kmeans_silhouette}")
print(f"DBSCAN Silhouette Score: {dbscan_silhouette}")

# Visualization (example)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(df_pca['PC1'], df_pca['PC2'], c=kmeans_labels, cmap='viridis')
plt.title('K-Means Clustering')

plt.subplot(1, 2, 2)
plt.scatter(df_pca['PC1'], df_pca['PC2'], c=dbscan_labels, cmap='viridis')
plt.title('DBSCAN Clustering')
plt.show() # To see the output, run the code.

"""#**Supervised Machine Learning: Classification & Regression**

"""

!pip install scikit-learn xgboost
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# --- Data Preparation ---
df_mdt = pd.merge(df_mdt, df_cat_encoded[['sg39_country_code_le', 'sg39_port_code_le', 'sg39_shipping_method_code_le']], left_index=True, right_index=True, how='left')

# Select features (X) and target variable (y)
X = df_mdt[['sg39_quantity', 'sg39_value', 'sg39_weight', 'sg39_country_code_le', 'sg39_port_code_le', 'sg39_shipping_method_code_le']]
y = df_mdt['sg39_category']# Convert categorical target to numerical if needed (e.g., using LabelEncoder)
# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()
# y = le.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# --- Model Training and Evaluation ---

# 1. Logistic Regression (LR)
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# 2. Decision Tree (DT)
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)

# 3. Support Vector Machine (SVM)
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# 4. K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)

# 5. Random Forest (RF)
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# --- Model Comparison ---

def evaluate_model(model_name, predictions, y_true):
    accuracy = accuracy_score(y_true, predictions)
    report = classification_report(y_true, predictions)
    conf_matrix = confusion_matrix(y_true, predictions)
    print(f"--- {model_name} ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Classification Report:\n{report}")
    print(f"Confusion Matrix:\n{conf_matrix}\n")

evaluate_model("Logistic Regression", lr_predictions, y_test)
evaluate_model("Decision Tree", dt_predictions, y_test)
evaluate_model("Support Vector Machine", svm_predictions, y_test)
evaluate_model("K-Nearest Neighbors", knn_predictions, y_test)
evaluate_model("Random Forest", rf_predictions, y_test)

# --- Specific Comparisons ---

# Regression-based Model Comparisons: LR vs SVM
print("--- LR vs SVM ---")
print(f"LR Accuracy: {accuracy_score(y_test, lr_predictions):.4f}")
print(f"SVM Accuracy: {accuracy_score(y_test, svm_predictions):.4f}\n")

# Non-Parametric Model Comparisons: DT vs KNN
print("--- DT vs KNN ---")
print(f"DT Accuracy: {accuracy_score(y_test, dt_predictions):.4f}")
print(f"KNN Accuracy: {accuracy_score(y_test, knn_predictions):.4f}\n")

# Cross Comparison of Models: LR vs DT
print("--- LR vs DT ---")
print(f"LR Accuracy: {accuracy_score(y_test, lr_predictions):.4f}")
print(f"DT Accuracy: {accuracy_score(y_test, dt_predictions):.4f}\n")

# Comparison of Base Model with Ensemble Model: DT vs RF
print("--- DT vs RF ---")
print(f"DT Accuracy: {accuracy_score(y_test, dt_predictions):.4f}")
print(f"RF Accuracy: {accuracy_score(y_test, rf_predictions):.4f}")

"""
# Diagnostics
### Unsupervised Machine Learning: Cluster Characteristics
# 1. Cluster Nature (Homogeneity):"""

import pandas as pd
from sklearn.cluster import KMeans
# Analyze cluster characteristics
for cluster in range(k):  # k is the number of clusters
    cluster_data = df_pca[df_pca['kmeans_cluster'] == cluster]
    print(f"--- Cluster {cluster} ---")
    print(cluster_data.describe())

"""# **2. Difference between Clusters (Heterogeneity):**"""

# Compare means of PC1 across clusters # Changed 'feature1' to 'PC1'
for cluster in range(k):
    mean_feature1 = cluster_data[cluster_data['kmeans_cluster'] == cluster]['PC1'].mean()
    print(f"Cluster {cluster} - Mean of PC1: {mean_feature1}") # Changed print statement

"""# **Supervised Machine Learning: Key Input Variables**
# **1. Feature Importance:**
"""

from sklearn.ensemble import RandomForestClassifier

# Get feature importances
importances = rf_model.feature_importances_
feature_names = X.columns  # X is your feature matrix

# Create a DataFrame for easier viewing
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print(importance_df)

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming X and y are your features and target variable

# Create a Decision Tree with size control parameters
dt_model = DecisionTreeClassifier(
    max_depth=3,        # Limit depth to 3 levels
    min_samples_split=30,  # Require at least 50 samples to split a node
    min_samples_leaf=15,   # Require at least 25 samples in a leaf
    random_state=42
)

dt_model.fit(X, y)

# Visualize the decision tree
plt.figure(figsize=(10, 6))
plot_tree(dt_model, feature_names=X.columns, filled=True)
plt.show()